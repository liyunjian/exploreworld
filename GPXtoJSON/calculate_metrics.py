#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPXè½¨è¿¹æ•°æ®é¢„å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼š
1. è®¡ç®—roadç›®å½•ä¸‹æ‰€æœ‰GPXæ–‡ä»¶çš„æ¢ç´¢é¢ç§¯å’Œç™¾åˆ†æ¯”
2. å°†æ‰€æœ‰ç±»å‹çš„GPXè½¨è¿¹è½¬æ¢ä¸ºGeoJSONæ ¼å¼
3. ç”Ÿæˆè½¨è¿¹ç¼“å­˜æ–‡ä»¶ä¾›ç½‘é¡µå¿«é€ŸåŠ è½½
ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶ä¾›ç½‘é¡µè°ƒç”¨
"""

import os
import json
import xml.etree.ElementTree as ET
import math
import glob
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import multiprocessing

# åœ°çƒè¡¨é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰
EARTH_SURFACE_AREA = 510072000e6

# æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆå­—èŠ‚ï¼‰
MAX_FILE_SIZE = 20 * 1024 * 1024  # 20MB

# è½¨è¿¹ç±»å‹é…ç½®
GPX_TYPES = {
    'road': {'color': '#ef4444', 'display_type': 'points'},
    'train': {'color': '#10b981', 'display_type': 'lines'},
    'plane': {'color': '#3b82f6', 'display_type': 'lines'},
    'other': {'color': '#f59e0b', 'display_type': 'lines'}
}

def haversine(lat1, lon1, lat2, lon2):
    """
    è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„å“ˆå¼—è¾›è·ç¦»ï¼ˆç±³ï¼‰
    """
    R = 6371e3  # åœ°çƒåŠå¾„ï¼ˆç±³ï¼‰
    lat1_rad = math.radians(lat1)
    lon1_rad = math.radians(lon1)
    lat2_rad = math.radians(lat2)
    lon2_rad = math.radians(lon2)
    
    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad
    
    a = (math.sin(dlat/2)**2 + 
         math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2)
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    
    return R * c

def get_json_size(data):
    """
    ä¼°ç®—JSONæ•°æ®åºåˆ—åŒ–åçš„å­—èŠ‚å¤§å°ï¼ˆä½¿ç”¨å®é™…ä¿å­˜æ ¼å¼ï¼‰
    """
    return len(json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8'))

def create_cache_directory():
    """
    åˆ›å»ºç¼“å­˜ç›®å½•
    """
    cache_dir = "cache"
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
        print(f"åˆ›å»ºç¼“å­˜ç›®å½•: {cache_dir}")
    return cache_dir

def save_to_json_with_size_limit(data, base_filename, cache_dir):
    """
    ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶ï¼Œæ”¯æŒå¤§å°é™åˆ¶å’Œæ–‡ä»¶åˆ†ç‰‡
    """
    file_info = []
    
    # æ£€æŸ¥å•ä¸ªæ–‡ä»¶æ˜¯å¦è¶…è¿‡å¤§å°é™åˆ¶
    total_size = get_json_size(data)
    print(f"æ€»æ•°æ®å¤§å°: {total_size / 1024 / 1024:.2f} MB")
    
    if total_size <= MAX_FILE_SIZE:
        # æ–‡ä»¶å¤§å°åœ¨é™åˆ¶å†…ï¼Œç›´æ¥ä¿å­˜
        filepath = os.path.join(cache_dir, f"{base_filename}.json")
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ä¿å­˜åˆ°å•ä¸ªæ–‡ä»¶: {filepath} ({total_size / 1024 / 1024:.2f} MB)")
            
            # ç”Ÿæˆé™æ€é…ç½®æ–‡ä»¶ï¼ˆå•ä¸ªæ–‡ä»¶ï¼‰
            config_data = {
                "data_type": "single",
                "single_file": f"{base_filename}.json",
                "total_size_mb": total_size / 1024 / 1024,
                "generated_at": data.get("generated_at")
            }
            
            config_filepath = os.path.join(cache_dir, "data_config.json")
            try:
                with open(config_filepath, 'w', encoding='utf-8') as f:
                    json.dump(config_data, f, ensure_ascii=False, indent=2)
                print(f"âœ… ç”Ÿæˆé™æ€é…ç½®æ–‡ä»¶: data_config.json")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆé…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
            return [{"file": f"{base_filename}.json", "size_mb": total_size / 1024 / 1024}]
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
            return []
    else:
        # æ–‡ä»¶è¿‡å¤§ï¼Œéœ€è¦åˆ†ç‰‡
        print(f"âš ï¸ æ–‡ä»¶è¿‡å¤§ ({total_size / 1024 / 1024:.2f} MB > {MAX_FILE_SIZE / 1024 / 1024} MB)ï¼Œå¼€å§‹åˆ†ç‰‡...")
        
        # æŒ‰è½¨è¿¹ç±»å‹åˆ†ç‰‡
        for i, (track_type, track_data) in enumerate(data.get("tracks", {}).items()):
            # æ¯ä¸ªè½¨è¿¹ç±»å‹å•ç‹¬ä¿å­˜
            chunk_data = {
                "metrics": data.get("metrics", {}),
                "tracks": {track_type: track_data},
                "bounds": data.get("bounds"),
                "generated_at": data.get("generated_at"),
                "version": data.get("version"),
                "chunk_info": {
                    "chunk_id": i,
                    "track_type": track_type,
                    "is_chunk": True
                }
            }
            
            chunk_size = get_json_size(chunk_data)
            chunk_filename = f"{base_filename}_chunk_{i}_{track_type}.json"
            chunk_filepath = os.path.join(cache_dir, chunk_filename)
            
            try:
                with open(chunk_filepath, 'w', encoding='utf-8') as f:
                    json.dump(chunk_data, f, ensure_ascii=False, indent=2)
                print(f"âœ… ä¿å­˜åˆ†ç‰‡æ–‡ä»¶: {chunk_filename} ({chunk_size / 1024 / 1024:.2f} MB)")
                file_info.append({
                    "file": chunk_filename, 
                    "size_mb": chunk_size / 1024 / 1024,
                    "track_type": track_type,
                    "chunk_id": i
                })
            except Exception as e:
                print(f"âŒ ä¿å­˜åˆ†ç‰‡æ–‡ä»¶ {chunk_filepath} æ—¶å‡ºé”™: {e}")
        
        # ç”Ÿæˆä¸»ç´¢å¼•æ–‡ä»¶
        index_data = {
            "total_size_mb": total_size / 1024 / 1024,
            "chunks": file_info,
            "generated_at": data.get("generated_at"),
            "version": data.get("version"),
            "is_chunked": True
        }
        
        index_filepath = os.path.join(cache_dir, f"{base_filename}_index.json")
        try:
            with open(index_filepath, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç”Ÿæˆç´¢å¼•æ–‡ä»¶: {base_filename}_index.json")
            file_info.insert(0, {"file": f"{base_filename}_index.json", "size_mb": get_json_size(index_data) / 1024 / 1024, "is_index": True})
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç´¢å¼•æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        # ç”Ÿæˆé™æ€é…ç½®æ–‡ä»¶ï¼ˆç”¨äºé™æ€æ‰˜ç®¡ï¼‰
        config_data = {
            "data_type": "chunked",
            "chunks": [chunk["file"] for chunk in file_info if not chunk.get("is_index")],
            "index_file": f"{base_filename}_index.json",
            "total_size_mb": total_size / 1024 / 1024,
            "generated_at": data.get("generated_at")
        }
        
        config_filepath = os.path.join(cache_dir, "data_config.json")
        try:
            with open(config_filepath, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç”Ÿæˆé™æ€é…ç½®æ–‡ä»¶: data_config.json")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆé…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return file_info

def parse_gpx_file_fast(file_path):
    """
    å¿«é€Ÿè§£æGPXæ–‡ä»¶ï¼Œæå–è½¨è¿¹ç‚¹åæ ‡ï¼Œå‡å°‘é‡å¤æŸ¥æ‰¾
    """
    points = []
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        # é¢„ç¼–è¯‘å‘½åç©ºé—´è·¯å¾„ï¼Œæ”¯æŒ GPX 1.0 å’Œ 1.1 ç‰ˆæœ¬
        namespaced_path_v11 = './/{http://www.topografix.com/GPX/1/1}trkpt'
        namespaced_path_v10 = './/{http://www.topografix.com/GPX/1/0}trkpt'
        simple_path = './/trkpt'
        
        # å°è¯•ä¸åŒçš„å‘½åç©ºé—´è·¯å¾„
        trkpts = root.findall(namespaced_path_v11)
        if not trkpts:
            trkpts = root.findall(namespaced_path_v10)
        if not trkpts:
            trkpts = root.findall(simple_path)
        
        # æ‰¹é‡å¤„ç†ï¼Œå‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
        for trkpt in trkpts:
            try:
                lat = float(trkpt.get('lat'))
                lon = float(trkpt.get('lon'))
                points.append((lat, lon))
            except (ValueError, TypeError):
                continue  # è·³è¿‡æ— æ•ˆç‚¹
            
        print(f"  å¿«é€Ÿè§£æ {os.path.basename(file_path)}: {len(points)} ä¸ªè½¨è¿¹ç‚¹")
        return points
        
    except Exception as e:
        print(f"  é”™è¯¯ï¼šè§£æ {file_path} æ—¶å‡ºé”™: {e}")
        return []

def fix_dateline_crossing(coordinates):
    """
    ä¿®å¤è·¨è¶Šæ—¥ç•Œçº¿çš„åæ ‡åºåˆ—
    å¦‚æœç›¸é‚»ä¸¤ç‚¹ç»åº¦å·®è¶…è¿‡180åº¦ï¼Œåˆ™è°ƒæ•´åæ ‡ä½¿è½¨è¿¹æ­£ç¡®æ˜¾ç¤º
    """
    if len(coordinates) < 2:
        return coordinates
    
    fixed_coordinates = [coordinates[0]]  # ä¿ç•™ç¬¬ä¸€ä¸ªç‚¹
    
    for i in range(1, len(coordinates)):
        prev_lon = fixed_coordinates[-1][0]
        curr_lon = coordinates[i][0]
        curr_lat = coordinates[i][1]
        
        # è®¡ç®—ç»åº¦å·®
        lon_diff = curr_lon - prev_lon
        
        # å¦‚æœç»åº¦å·®è¶…è¿‡180åº¦ï¼Œè¯´æ˜è·¨è¶Šäº†æ—¥ç•Œçº¿
        if lon_diff > 180:
            # ä»ä¸œå‘è¥¿è·¨è¶Šæ—¥ç•Œçº¿ï¼Œå°†å½“å‰ç‚¹çš„ç»åº¦å‡360åº¦
            fixed_coordinates.append([curr_lon - 360, curr_lat])
        elif lon_diff < -180:
            # ä»è¥¿å‘ä¸œè·¨è¶Šæ—¥ç•Œçº¿ï¼Œå°†å½“å‰ç‚¹çš„ç»åº¦åŠ 360åº¦
            fixed_coordinates.append([curr_lon + 360, curr_lat])
        else:
            # æ­£å¸¸æƒ…å†µï¼Œä¸éœ€è¦è°ƒæ•´
            fixed_coordinates.append([curr_lon, curr_lat])
    
    return fixed_coordinates

def parse_gpx_to_geojson(file_path, track_type='road'):
    """
    å°†GPXæ–‡ä»¶è§£æä¸ºGeoJSONæ ¼å¼
    è¿”å›ç‚¹å’Œçº¿çš„GeoJSONæ•°æ®
    """
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        points_features = []
        line_features = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è½¨è¿¹æ®µï¼Œæ”¯æŒ GPX 1.0 å’Œ 1.1 ç‰ˆæœ¬
        trksegs = root.findall('.//{http://www.topografix.com/GPX/1/1}trkseg')
        if not trksegs:
            trksegs = root.findall('.//{http://www.topografix.com/GPX/1/0}trkseg')
        if not trksegs:
            trksegs = root.findall('.//trkseg')
        
        for trkseg in trksegs:
            coordinates = []
            
            # å¤„ç†è½¨è¿¹æ®µä¸­çš„æ¯ä¸ªç‚¹ï¼Œæ”¯æŒ GPX 1.0 å’Œ 1.1 ç‰ˆæœ¬
            trkpts = trkseg.findall('{http://www.topografix.com/GPX/1/1}trkpt')
            if not trkpts:
                trkpts = trkseg.findall('{http://www.topografix.com/GPX/1/0}trkpt')
            if not trkpts:
                trkpts = trkseg.findall('trkpt')
            
            for trkpt in trkpts:
                lat = float(trkpt.get('lat'))
                lon = float(trkpt.get('lon'))
                coordinates.append([lon, lat])  # GeoJSONæ ¼å¼æ˜¯[lon, lat]
                
                # ä¸ºæ¯ä¸ªç‚¹åˆ›å»ºPointç‰¹å¾
                points_features.append({
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [lon, lat]
                    },
                    "properties": {
                        "track_type": track_type
                    }
                })
            
            # å¦‚æœæœ‰å¤šä¸ªç‚¹ï¼Œåˆ›å»ºLineStringç‰¹å¾
            if len(coordinates) > 1:
                # ä¿®å¤è·¨æ—¥ç•Œçº¿çš„åæ ‡
                fixed_coordinates = fix_dateline_crossing(coordinates)
                
                line_features.append({
                    "type": "Feature",
                    "geometry": {
                        "type": "LineString",
                        "coordinates": fixed_coordinates
                    },
                    "properties": {
                        "track_type": track_type
                    }
                })
        
        return {
            "points": {
                "type": "FeatureCollection",
                "features": points_features
            },
            "lines": {
                "type": "FeatureCollection",
                "features": line_features
            }
        }
        
    except Exception as e:
        print(f"  é”™è¯¯ï¼šè§£æ {file_path} ä¸ºGeoJSONæ—¶å‡ºé”™: {e}")
        return {
            "points": {"type": "FeatureCollection", "features": []},
            "lines": {"type": "FeatureCollection", "features": []}
        }

def calculate_grid_area_fast(points, grid_size_meters=50):
    """
    ä½¿ç”¨é›†åˆå¿«é€Ÿè®¡ç®—ç½‘æ ¼åŒ–é¢ç§¯ï¼Œæ—¶é—´å¤æ‚åº¦O(n)
    """
    if not points:
        return 0, 0
    
    # ä½¿ç”¨é›†åˆå­˜å‚¨ç½‘æ ¼åæ ‡ï¼Œè‡ªåŠ¨å»é‡
    grid_cells = set()
    
    for lat, lon in points:
        # è®¡ç®—ç½‘æ ¼åæ ‡
        lat_degrees_per_grid = grid_size_meters / 111000
        lon_degrees_per_grid = grid_size_meters / (111000 * math.cos(math.radians(lat)))
        
        # å°†åæ ‡æ˜ å°„åˆ°ç½‘æ ¼
        grid_lat = int(lat / lat_degrees_per_grid)
        grid_lon = int(lon / lon_degrees_per_grid)
        
        grid_cells.add((grid_lat, grid_lon))
    
    # è®¡ç®—æ€»é¢ç§¯
    total_area_m2 = len(grid_cells) * (grid_size_meters ** 2)
    
    return len(grid_cells), total_area_m2

def remove_duplicates_fast(points, min_distance=50):
    """
    ä½¿ç”¨ç©ºé—´ç½‘æ ¼ç´¢å¼•å¿«é€Ÿå»é‡ï¼Œæ—¶é—´å¤æ‚åº¦O(n)
    """
    if not points:
        return []
    
    # è®¡ç®—ç½‘æ ¼å¤§å°ï¼ˆç¨å¤§äºmin_distanceä»¥ç¡®ä¿è¦†ç›–ï¼‰
    grid_size = min_distance * 1.5  # 75ç±³ç½‘æ ¼
    grid_dict = {}
    unique_points = []
    total_points = len(points)
    
    print(f"  ä½¿ç”¨ {grid_size}m ç½‘æ ¼åŠ é€Ÿå»é‡...")
    
    for i, (lat, lon) in enumerate(points):
        # è®¡ç®—ç½‘æ ¼åæ ‡
        grid_lat = int(lat * 111000 / grid_size)  # çº¬åº¦ç½‘æ ¼
        grid_lon = int(lon * 111000 * math.cos(math.radians(lat)) / grid_size)  # ç»åº¦ç½‘æ ¼
        
        # æ£€æŸ¥å½“å‰ç½‘æ ¼å’Œç›¸é‚»8ä¸ªç½‘æ ¼
        is_duplicate = False
        for dlat in [-1, 0, 1]:
            for dlon in [-1, 0, 1]:
                check_grid = (grid_lat + dlat, grid_lon + dlon)
                if check_grid in grid_dict:
                    # åªæ£€æŸ¥åŒç½‘æ ¼å†…çš„ç‚¹
                    for existing_lat, existing_lon in grid_dict[check_grid]:
                        if haversine(lat, lon, existing_lat, existing_lon) <= min_distance:
                            is_duplicate = True
                            break
                if is_duplicate:
                    break
            if is_duplicate:
                break
        
        if not is_duplicate:
            unique_points.append((lat, lon))
            # æ·»åŠ åˆ°ç½‘æ ¼å­—å…¸
            grid_key = (grid_lat, grid_lon)
            if grid_key not in grid_dict:
                grid_dict[grid_key] = []
            grid_dict[grid_key].append((lat, lon))
        
        # æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 5000 == 0 or i == total_points - 1:
            print(f"  å¿«é€Ÿå»é‡è¿›åº¦: {i+1}/{total_points} -> {len(unique_points)} ä¸ªæœ‰æ•ˆç‚¹")
    
    return unique_points

def remove_duplicates(points, min_distance=50):
    """
    å»é™¤é‡å¤ç‚¹ï¼ˆè·ç¦»å°äºmin_distanceç±³çš„ç‚¹åªä¿ç•™ä¸€ä¸ªï¼‰
    """
    unique_points = []
    total_points = len(points)
    
    for i, point in enumerate(points):
        is_duplicate = False
        for existing in unique_points:
            if haversine(point[0], point[1], existing[0], existing[1]) <= min_distance:
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_points.append(point)
        
        # æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 5000 == 0 or i == total_points - 1:
            print(f"  å»é‡è¿›åº¦: {i+1}/{total_points} -> {len(unique_points)} ä¸ªæœ‰æ•ˆç‚¹")
    
    return unique_points

def process_road_data():
    """
    ç»Ÿä¸€å¤„ç†roadç±»å‹çš„GPXæ•°æ®ï¼Œè¿›è¡Œå»é‡å’Œé¢ç§¯è®¡ç®—
    è¿”å›å»é‡åçš„ç‚¹åæ ‡å’Œè®¡ç®—ç»“æœ
    """
    print("\n=== å¤„ç†roadè½¨è¿¹æ•°æ® ===")
    
    # æŸ¥æ‰¾GPX/roadç›®å½•ä¸‹çš„æ‰€æœ‰GPXæ–‡ä»¶
    gpx_pattern = os.path.join("GPX", "road", "*.gpx")
    gpx_files = glob.glob(gpx_pattern)
    
    if not gpx_files:
        print("è­¦å‘Šï¼šåœ¨ GPX/road/ ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•GPXæ–‡ä»¶")
        return [], {
            "calculation_time": datetime.now().isoformat(),
            "total_files": 0,
            "total_points": 0,
            "unique_points": 0,
            "grid_cells": 0,
            "grid_size_meters": 50,
            "explored_area_km2": 0,
            "earth_percentage": 0,
            "calculation_method": "grid_based",
            "files": []
        }
    
    print(f"æ‰¾åˆ° {len(gpx_files)} ä¸ªGPXæ–‡ä»¶:")
    for file in gpx_files:
        print(f"  - {os.path.basename(file)}")
    
    # å¹¶è¡Œå¤„ç†å¤šä¸ªGPXæ–‡ä»¶
    print(f"ä½¿ç”¨ {min(len(gpx_files), 4)} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†æ–‡ä»¶...")
    all_points = []
    
    with ThreadPoolExecutor(max_workers=min(len(gpx_files), 4)) as executor:
        future_to_file = {executor.submit(parse_gpx_file_fast, gpx_file): gpx_file 
                         for gpx_file in gpx_files}
        
        for future in future_to_file:
            points = future.result()
            all_points.extend(points)
    
    print(f"\næ€»å…±æ”¶é›†åˆ° {len(all_points)} ä¸ªè½¨è¿¹ç‚¹")
    
    if not all_points:
        print("é”™è¯¯ï¼šæ²¡æœ‰è§£æåˆ°ä»»ä½•è½¨è¿¹ç‚¹")
        return [], None
    
    # ä¸€æ¬¡æ€§å®Œæˆå»é‡è®¡ç®—
    print("å¼€å§‹å¿«é€Ÿå»é‡è®¡ç®—...")
    unique_points = remove_duplicates_fast(all_points)
    
    # ä½¿ç”¨å»é‡åçš„ç‚¹è®¡ç®—ç½‘æ ¼åŒ–é¢ç§¯
    print("è®¡ç®—ç½‘æ ¼åŒ–æ¢ç´¢é¢ç§¯...")
    grid_count, total_explored_area_m2 = calculate_grid_area_fast(unique_points, grid_size_meters=50)
    explored_area_km2 = total_explored_area_m2 / 1e6
    percentage = (total_explored_area_m2 / EARTH_SURFACE_AREA) * 100
    
    result = {
        "calculation_time": datetime.now().isoformat(),
        "total_files": len(gpx_files),
        "total_points": len(all_points),
        "unique_points": len(unique_points),
        "grid_cells": grid_count,
        "grid_size_meters": 50,
        "explored_area_km2": round(explored_area_km2, 6),
        "earth_percentage": round(percentage, 15),
        "calculation_method": "grid_based",
        "files": [os.path.basename(f) for f in gpx_files]
    }
    
    print(f"\n=== è®¡ç®—ç»“æœ ===")
    print(f"å¤„ç†æ–‡ä»¶æ•°: {result['total_files']}")
    print(f"æ€»è½¨è¿¹ç‚¹: {result['total_points']}")
    print(f"å»é‡åç‚¹æ•°: {result['unique_points']}")
    print(f"ç½‘æ ¼å•å…ƒæ•°: {result['grid_cells']}")
    print(f"ç½‘æ ¼å¤§å°: {result['grid_size_meters']}m Ã— {result['grid_size_meters']}m")
    print(f"æ¢ç´¢é¢ç§¯: {result['explored_area_km2']} kmÂ²")
    print(f"å åœ°çƒæ¯”ä¾‹: {result['earth_percentage']}%")
    print(f"è®¡ç®—æ–¹æ³•: {result['calculation_method']}")
    
    return unique_points, result

def generate_tracks_data(road_unique_points=None):
    """
    ç”Ÿæˆæ‰€æœ‰è½¨è¿¹çš„GeoJSONæ•°æ®
    road_unique_points: é¢„å¤„ç†çš„roadè½¨è¿¹å»é‡ç‚¹ï¼Œé¿å…é‡å¤è®¡ç®—
    """
    print("\n=== ç”Ÿæˆè½¨è¿¹æ•°æ® ===")
    
    tracks_data = {}
    all_coordinates = []  # ç”¨äºè®¡ç®—è¾¹ç•Œ
    
    for track_type in GPX_TYPES.keys():
        print(f"\nå¤„ç† {track_type} è½¨è¿¹...")
        
        if track_type == 'road' and road_unique_points is not None:
            # ä½¿ç”¨é¢„å¤„ç†çš„roadæ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
            print(f"  ä½¿ç”¨é¢„å¤„ç†çš„roadæ•°æ®: {len(road_unique_points)} ä¸ªå»é‡ç‚¹")
            
            all_points_features = []
            for lat, lon in road_unique_points:
                all_points_features.append({
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [lon, lat]
                    },
                    "properties": {
                        "track_type": track_type
                    }
                })
                all_coordinates.append([lon, lat])
            
            # å¯¹äºroadç±»å‹ï¼Œæˆ‘ä»¬ä¸»è¦å±•ç¤ºç‚¹ï¼Œçº¿è¦ç´ å¯ä»¥ä¸ºç©ºæˆ–è€…ä»åŸå§‹æ•°æ®ç”Ÿæˆ
            gpx_pattern = os.path.join("GPX", track_type, "*.gpx")
            gpx_files = glob.glob(gpx_pattern)
            all_lines_features = []
            
            # åªç”Ÿæˆçº¿è¦ç´ ï¼Œä¸é‡å¤å¤„ç†ç‚¹
            for gpx_file in gpx_files:
                geojson_data = parse_gpx_to_geojson(gpx_file, track_type)
                all_lines_features.extend(geojson_data["lines"]["features"])
            
            tracks_data[track_type] = {
                "points": {
                    "type": "FeatureCollection",
                    "features": all_points_features
                },
                "lines": {
                    "type": "FeatureCollection", 
                    "features": all_lines_features
                },
                "files": [os.path.basename(f) for f in gpx_files],
                "color": GPX_TYPES[track_type]['color'],
                "display_type": GPX_TYPES[track_type]['display_type'],
                "points_count": len(all_points_features),
                "lines_count": len(all_lines_features)
            }
            
            print(f"  å®Œæˆ: {len(all_points_features)} ä¸ªç‚¹, {len(all_lines_features)} æ¡çº¿")
            continue
        
        # å¤„ç†å…¶ä»–ç±»å‹çš„è½¨è¿¹
        gpx_pattern = os.path.join("GPX", track_type, "*.gpx")
        gpx_files = glob.glob(gpx_pattern)
        
        if not gpx_files:
            print(f"  {track_type} ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°GPXæ–‡ä»¶")
            tracks_data[track_type] = {
                "points": {"type": "FeatureCollection", "features": []},
                "lines": {"type": "FeatureCollection", "features": []},
                "files": [],
                "color": GPX_TYPES[track_type]['color'],
                "display_type": GPX_TYPES[track_type]['display_type'],
                "points_count": 0,
                "lines_count": 0
            }
            continue
        
        print(f"  æ‰¾åˆ° {len(gpx_files)} ä¸ªæ–‡ä»¶: {[os.path.basename(f) for f in gpx_files]}")
        
        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„GeoJSONæ•°æ®
        all_points_features = []
        all_lines_features = []
        all_raw_points = []  # ç”¨äºå»é‡çš„åŸå§‹ç‚¹åæ ‡
        
        for gpx_file in gpx_files:
            geojson_data = parse_gpx_to_geojson(gpx_file, track_type)
            all_points_features.extend(geojson_data["points"]["features"])
            all_lines_features.extend(geojson_data["lines"]["features"])
            
            # æ”¶é›†åŸå§‹åæ ‡ç”¨äºå»é‡
            for feature in geojson_data["points"]["features"]:
                coord = feature["geometry"]["coordinates"]
                all_raw_points.append((coord[1], coord[0]))  # è½¬æ¢ä¸º(lat, lon)æ ¼å¼
        
        # å¯¹ç‚¹è¿›è¡Œå»é‡å¤„ç†
        if all_raw_points:
            print(f"  åŸå§‹ç‚¹æ•°: {len(all_raw_points)}")
            unique_raw_points = remove_duplicates_fast(all_raw_points, min_distance=50)
            print(f"  å»é‡åç‚¹æ•°: {len(unique_raw_points)}")
            
            # é‡æ–°ç”Ÿæˆå»é‡åçš„GeoJSONç‚¹è¦ç´ 
            all_points_features = []
            for lat, lon in unique_raw_points:
                all_points_features.append({
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [lon, lat]
                    },
                    "properties": {
                        "track_type": track_type
                    }
                })
                all_coordinates.append([lon, lat])
        
        # ä»çº¿è¦ç´ ä¸­ä¹Ÿæ”¶é›†åæ ‡ç”¨äºè®¡ç®—è¾¹ç•Œ
        for feature in all_lines_features:
            if feature["geometry"]["type"] == "LineString":
                for coord in feature["geometry"]["coordinates"]:
                    all_coordinates.append(coord)
        
        tracks_data[track_type] = {
            "points": {
                "type": "FeatureCollection",
                "features": all_points_features
            },
            "lines": {
                "type": "FeatureCollection", 
                "features": all_lines_features
            },
            "files": [os.path.basename(f) for f in gpx_files],
            "color": GPX_TYPES[track_type]['color'],
            "display_type": GPX_TYPES[track_type]['display_type'],
            "points_count": len(all_points_features),
            "lines_count": len(all_lines_features)
        }
        
        print(f"  å®Œæˆ: {len(all_points_features)} ä¸ªç‚¹, {len(all_lines_features)} æ¡çº¿")
    
    # è®¡ç®—è¾¹ç•Œ
    bounds = None
    if all_coordinates:
        lons = [coord[0] for coord in all_coordinates]
        lats = [coord[1] for coord in all_coordinates]
        bounds = {
            "min_lng": min(lons),
            "max_lng": max(lons), 
            "min_lat": min(lats),
            "max_lat": max(lats)
        }
        print(f"\nè®¡ç®—å¾—åˆ°è½¨è¿¹è¾¹ç•Œ: {bounds}")
    
    return tracks_data, bounds

def save_to_json(data, filename="metrics.json"):
    """
    ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return True
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    print("=== GPXè½¨è¿¹æ•°æ®é¢„å¤„ç†å·¥å…· ===")
    print("åŠŸèƒ½ï¼š1. è®¡ç®—æ¢ç´¢é¢ç§¯  2. ç”Ÿæˆè½¨è¿¹ç¼“å­˜ï¼ˆæ”¯æŒå¤§æ–‡ä»¶åˆ†ç‰‡ï¼‰")
    
    # æ£€æŸ¥GPXç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists("GPX"):
        print("é”™è¯¯ï¼šGPX ç›®å½•ä¸å­˜åœ¨")
        exit(1)
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    cache_dir = create_cache_directory()
    
    # 1. ç»Ÿä¸€å¤„ç†roadè½¨è¿¹æ•°æ®ï¼ˆå»é‡ + é¢ç§¯è®¡ç®—ï¼‰
    road_unique_points, metrics = process_road_data()
    
    if not metrics:
        print("\nâŒ æ¢ç´¢é¢ç§¯è®¡ç®—å¤±è´¥ï¼")
        exit(1)
    
    # 2. ç”Ÿæˆæ‰€æœ‰è½¨è¿¹æ•°æ®ï¼ˆä½¿ç”¨é¢„å¤„ç†çš„roadæ•°æ®ï¼‰
    print("\nå¼€å§‹ç”Ÿæˆè½¨è¿¹æ•°æ®...")
    tracks_data, bounds = generate_tracks_data(road_unique_points)
    
    # 3. åˆå¹¶æ‰€æœ‰æ•°æ®
    final_data = {
        "metrics": metrics,
        "tracks": tracks_data,
        "bounds": bounds,
        "generated_at": datetime.now().isoformat(),
        "version": "2.2"
    }
    
    # 4. ä¿å­˜æ•°æ®æ–‡ä»¶
    print(f"\n=== ä¿å­˜æ•°æ®æ–‡ä»¶åˆ°ç¼“å­˜ç›®å½• ({cache_dir}) ===")
    
    # ä¿å­˜æŒ‡æ ‡æ–‡ä»¶ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    metrics_file = os.path.join(cache_dir, "metrics.json")
    if save_to_json(metrics, metrics_file):
        print("âœ… æŒ‡æ ‡æ–‡ä»¶ä¿å­˜æˆåŠŸ")
    else:
        print("âŒ æŒ‡æ ‡æ–‡ä»¶ä¿å­˜å¤±è´¥")
        exit(1)
    
    # ä¿å­˜å®Œæ•´è½¨è¿¹æ•°æ®ï¼ˆæ”¯æŒå¤§æ–‡ä»¶åˆ†ç‰‡ï¼‰
    file_info = save_to_json_with_size_limit(final_data, "tracks_data", cache_dir)
    
    if file_info:
        print("âœ… è½¨è¿¹æ•°æ®æ–‡ä»¶ä¿å­˜æˆåŠŸ")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_points = sum(track.get('points_count', 0) for track in tracks_data.values())
        total_lines = sum(track.get('lines_count', 0) for track in tracks_data.values())
        total_files = sum(len(track.get('files', [])) for track in tracks_data.values())
        
        print(f"\n=== è½¨è¿¹æ•°æ®ç»Ÿè®¡ ===")
        print(f"å¤„ç†æ–‡ä»¶æ€»æ•°: {total_files}")
        print(f"è½¨è¿¹ç‚¹æ€»æ•°: {total_points}")
        print(f"è½¨è¿¹çº¿æ€»æ•°: {total_lines}")
        
        for track_type, data in tracks_data.items():
            if data.get('files'):
                print(f"{track_type}: {len(data['files'])} ä¸ªæ–‡ä»¶, {data.get('points_count', 0)} ä¸ªç‚¹, {data.get('lines_count', 0)} æ¡çº¿")
        
        if bounds:
            print(f"è½¨è¿¹è¾¹ç•Œ: ç»åº¦ {bounds['min_lng']:.4f} ~ {bounds['max_lng']:.4f}, çº¬åº¦ {bounds['min_lat']:.4f} ~ {bounds['max_lat']:.4f}")
        
        print(f"\n=== ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ ===")
        for info in file_info:
            if info.get('is_index'):
                print(f"ğŸ“‹ ç´¢å¼•æ–‡ä»¶: {info['file']} ({info['size_mb']:.2f} MB)")
            elif 'track_type' in info:
                print(f"ğŸ§© åˆ†ç‰‡æ–‡ä»¶: {info['file']} ({info['size_mb']:.2f} MB) - {info['track_type']}")
            else:
                print(f"ğŸ“„ æ•°æ®æ–‡ä»¶: {info['file']} ({info['size_mb']:.2f} MB)")
        
        print(f"\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
        print(f"æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ° {cache_dir} ç›®å½•")
        print("ç°åœ¨éœ€è¦ä¿®æ”¹ index.html ä»¥ä»ç¼“å­˜ç›®å½•è¯»å–æ–‡ä»¶")
        
    else:
        print("âŒ è½¨è¿¹æ•°æ®æ–‡ä»¶ä¿å­˜å¤±è´¥")
        exit(1)